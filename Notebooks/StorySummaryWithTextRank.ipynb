{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming flove data has already been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading glove word embeddings\n",
    "emb_file = open(\"data/glove.6B.100d.txt\", 'r', encoding='utf-8')\n",
    "with emb_file:\n",
    "    word2vec = emb_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dictionary for the Glove word and its first 100 dimensions\n",
    "temp = [word_vec.split() for word_vec in word2vec ]\n",
    "word2vec_dict = {}\n",
    "for words in temp:\n",
    "    word2vec_dict[words[0]]=words[1:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all the files from the train dictionary\n",
    "files = os.listdir(\"data/story_summary/stories_text_summarization_dataset_train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to perform TextRank:\n",
    "    Prepare dataset of text.\n",
    "    Clean, sent tokenize, word tokenize, preprocess by removing stopwords and word vectorize.\n",
    "    sent vectorization\n",
    "    cosine similarity matrix\n",
    "    graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return re.sub(r\"[^a-zA-Z0-9 .?,\\\"\\'']\", \"\", contractions_re.sub(replace, s))\n",
    "\n",
    "def correct_text(passage):\n",
    "    return [expand_contractions(i) for i in sent_tokenize(passage.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe to hold the passage and its summary as highlight\n",
    "df = pd.DataFrame(columns=['content','summary'])\n",
    "for i,file_name in enumerate(files[:100]):\n",
    "    summary, content, next = \"\", \"\", False\n",
    "    with open(f'./data/story_summary/stories_text_summarization_dataset_train/{file_name}', 'r', encoding='utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            if len(line.replace(\"\\n\", \"\"))>0:\n",
    "                if line.find('@highlight')>=0:\n",
    "                    next=True\n",
    "                    continue\n",
    "                if next:\n",
    "                    summary += line.replace(\"@highlight\", \"\").replace(\"\\n\", \"\").strip().lower() + \". \"\n",
    "                else:\n",
    "                    next=False\n",
    "                    content += line.replace(\"\\n\", \"\").strip().lower()\n",
    "        df.loc[i] = {'content':correct_text(content), 'summary':summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[it has  it is official u.s. president barack ...</td>\n",
       "      <td>syrian official: obama climbed to the top of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[cnn  usain bolt rounded off the world champio...</td>\n",
       "      <td>usain bolt wins third gold of world championsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[kansas city, missouri cnn  the general servic...</td>\n",
       "      <td>the employee in agency's kansas city office is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[los angeles cnn  a medical doctor in vancouve...</td>\n",
       "      <td>new: a canadian doctor says she was part of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[cnn  police arrested another teen thursday, t...</td>\n",
       "      <td>another arrest made in gang rape outside calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  [it has  it is official u.s. president barack ...   \n",
       "1  [cnn  usain bolt rounded off the world champio...   \n",
       "2  [kansas city, missouri cnn  the general servic...   \n",
       "3  [los angeles cnn  a medical doctor in vancouve...   \n",
       "4  [cnn  police arrested another teen thursday, t...   \n",
       "\n",
       "                                             summary  \n",
       "0  syrian official: obama climbed to the top of t...  \n",
       "1  usain bolt wins third gold of world championsh...  \n",
       "2  the employee in agency's kansas city office is...  \n",
       "3  new: a canadian doctor says she was part of a ...  \n",
       "4  another arrest made in gang rape outside calif...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passage_vec_gen(sent_list):\n",
    "    sent_vec_list = []\n",
    "    stop_word_list=stopwords.words('english')\n",
    "    for sent in sent_list:\n",
    "        vec = np.array(np.zeros(100,))\n",
    "        for word in sent:\n",
    "            if word not in stop_word_list and word in word2vec_dict.keys():\n",
    "                vec = np.add(vec, np.asfarray(word2vec_dict[word]))\n",
    "        templ = vec/len(sent)\n",
    "        sent_vec_list.append(templ)\n",
    "    return sent_vec_list\n",
    "\n",
    "df['passage_sent_vec'] = [passage_vec_gen(sent_list) for sent_list in df.content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cosine_sim_mat(sent_vec):\n",
    "    sim_mat = np.zeros([len(sent_vec), len(sent_vec)])\n",
    "    for i in range(len(sent_vec)):\n",
    "        for j in range(len(sent_vec)):\n",
    "            if(i != j):\n",
    "                sim_mat[i][j] = cosine_similarity(sent_vec[i].reshape(1,100), sent_vec[j].reshape(1,100))[0,0]\n",
    "    return(sim_mat)\n",
    "\n",
    "df['sim_mat'] = [gen_cosine_sim_mat(sent_vec) for sent_vec in df.passage_sent_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(df.content[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = cosine_similarity(message_embeddings)\n",
    "nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(df.content[0])), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"the two leaders agreed that the international community must deliver a resolute message to the assad regime  and others who would consider using chemical weapons  that these crimes are unacceptable and those who violate this international norm will be held accountable by the world,\" the white house said.meanwhile, as uncertainty loomed over how congress would weigh in, u.s. military officials said they remained at the ready.5 key assertions u.s. intelligence report on syriasyria who wants what after chemical weapons horrorreactions mixed to obama\\'s speecha spokesman for the syrian national coalition said that the opposition group was disappointed by obama\\'s announcement. some u.s. lawmakers have called for immediate action while others warn of stepping into what could become a quagmire.some global leaders have expressed support, but the british parliament\\'s vote against military action earlier this week was a blow to obama\\'s hopes of getting strong backing from key nato allies.on saturday, obama proposed what he said would be a limited military action against syrian president bashar alassad. \"bergen  syria is a problem from hell for the u.s.obama \\'this menace must be confronted\\'obama\\'s senior advisers have debated the next steps to take, and the president\\'s comments saturday came amid mounting political pressure over the situation in syria.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([x[1] for x in ranked_sentences[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'syrian official: obama climbed to the top of the tree, \"doesn\\'t know how to get down\". obama sends a letter to the heads of the house and senate. obama to seek congressional approval on military action against syria. aim is to determine whether cw were used, not by whom, says u.n. spokesman. '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summary[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
